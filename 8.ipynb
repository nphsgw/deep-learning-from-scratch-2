{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 時系列の長さT=5,隠れ状態ベクトルの要素数H=4\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "# (5,)のベクトルを(5,4)の配列に変換\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "# コンテキストベクトル\n",
    "c = np.sum(t, axis=0)\n",
    "# 0軸目を消すように合計を求めている。\n",
    "print(c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 6[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 12[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 19[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 25[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 31[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 43[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 51[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 60[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 69[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 79[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 87[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 99[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 106[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 114[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 121[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 127[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 135[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 7[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 16[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 30[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 38[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 46[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 54[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 60[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 67[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 74[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 81[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 88[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 95[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 102[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 108[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 115[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 122[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 128[s] | loss 0.47\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "val acc 50.900%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 6[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 13[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 19[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 26[s] | loss 0.10\n",
      "| epoch 3 |  iter 101 / 351 | time 33[s] | loss 0.07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mセル2 を /home/yuki/clone/deep-learning-from-scratch-2/8.ipynb\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/8.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m acc_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/8.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epoch):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/8.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(x_train, t_train, max_epoch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, max_grad\u001b[39m=\u001b[39;49mmax_grad)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/8.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     correct_num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/8.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x_test)):\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/common/trainer.py:43\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# 勾配を求め、パラメータを更新\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(batch_x, batch_t)\n\u001b[0;32m---> 43\u001b[0m model\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     44\u001b[0m params, grads \u001b[39m=\u001b[39m remove_duplicate(\n\u001b[1;32m     45\u001b[0m     model\u001b[39m.\u001b[39mparams, model\u001b[39m.\u001b[39mgrads\n\u001b[1;32m     46\u001b[0m )  \u001b[39m# 共有された重みを1つに集約\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m max_grad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/ch07/seq2seq.py:113\u001b[0m, in \u001b[0;36mSeq2seq.backward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, dout\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    112\u001b[0m     dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax\u001b[39m.\u001b[39mbackward(dout)\n\u001b[0;32m--> 113\u001b[0m     dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mbackward(dout)\n\u001b[1;32m    114\u001b[0m     dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mbackward(dh)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m dout\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/ch08/attention_seq2seq.py:64\u001b[0m, in \u001b[0;36mAttentionDecoder.backward\u001b[0;34m(self, dscore)\u001b[0m\n\u001b[1;32m     62\u001b[0m denc_hs, ddec_hs1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mbackward(dc)\n\u001b[1;32m     63\u001b[0m ddec_hs \u001b[39m=\u001b[39m ddec_hs0 \u001b[39m+\u001b[39m ddec_hs1\n\u001b[0;32m---> 64\u001b[0m dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm\u001b[39m.\u001b[39;49mbackward(ddec_hs)\n\u001b[1;32m     65\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm\u001b[39m.\u001b[39mdh\n\u001b[1;32m     66\u001b[0m denc_hs[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dh\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/common/time_layers.py:211\u001b[0m, in \u001b[0;36mTimeLSTM.backward\u001b[0;34m(self, dhs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(T)):\n\u001b[1;32m    210\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[t]\n\u001b[0;32m--> 211\u001b[0m     dx, dh, dc \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(dhs[:, t, :] \u001b[39m+\u001b[39;49m dh, dc)\n\u001b[1;32m    212\u001b[0m     dxs[:, t, :] \u001b[39m=\u001b[39m dx\n\u001b[1;32m    213\u001b[0m     \u001b[39mfor\u001b[39;00m i, grad \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(layer\u001b[39m.\u001b[39mgrads):\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/common/time_layers.py:163\u001b[0m, in \u001b[0;36mLSTM.backward\u001b[0;34m(self, dh_next, dc_next)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrads[\u001b[39m2\u001b[39m][\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m=\u001b[39m db\n\u001b[1;32m    162\u001b[0m dx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dA, Wx\u001b[39m.\u001b[39mT)\n\u001b[0;32m--> 163\u001b[0m dh_prev \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(dA, Wh\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m dx, dh_prev, dc_prev\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pickletools import optimize\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from ch08.attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 入力分の反転\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(\n",
    "            model, question, correct, id_to_char, verbose, is_reverse=True\n",
    "        )\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print(\"val acc %.3f%%\" % (acc * 100))\n",
    "\n",
    "model.save_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

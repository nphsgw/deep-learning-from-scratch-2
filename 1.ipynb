{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]]\n",
      "[[ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]\n",
      " [ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]\n",
      " [ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]\n",
      " [ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]\n",
      " [ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]\n",
      " [ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]\n",
      " [ 0.2716913   0.31328554 -0.33179391  0.34225014 -0.20148623 -0.81429876\n",
      "   1.52860886  1.69701163]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# repeatノードの練習\n",
    "D, N = 8, 7\n",
    "\n",
    "# forwardの実装\n",
    "# 1行データをN行データにする\n",
    "# 1次元の配列を作成\n",
    "x = np.random.randn(1, D)\n",
    "print(x)\n",
    "# 1次元データを1行のデータとして、これをN行データにする。\n",
    "# 増えた行は1行データをコピーしたものを入れる\n",
    "# axisを指定することでどの軸方向に複製するかを指定する。\n",
    "y = np.repeat(x, N, axis=0)\n",
    "print(y)\n",
    "\n",
    "# backwardの実装\n",
    "# N行データを1行データにする。\n",
    "dy = np.random.randn(N, D)\n",
    "# keepdims=Trueにすると、2次元配列の次元数を維持する。\n",
    "# Trueだと(1,D)になる。Falesだと(D,)となる。\n",
    "dx = np.sum(dy, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.55112429  1.94546382  0.01693351 -0.40759367 -0.07287369 -1.57047863\n",
      "  -0.41548384 -0.35526037]\n",
      " [ 0.39682276  0.7640211  -0.21745515  0.15166842 -0.78898379 -1.41786413\n",
      "   1.01034943  0.12521147]\n",
      " [-0.67740824 -1.17617023  0.72255832  1.7014048  -1.36562217  1.6851131\n",
      "  -1.30418126  1.35705874]\n",
      " [ 0.947782   -0.31486945 -0.6594238  -0.5162289   0.18278371  0.56466281\n",
      "  -1.08844818  0.63771416]\n",
      " [-1.75597948 -0.18889256 -0.10846751  0.80069804  1.23564232 -0.38523916\n",
      "   2.2398292   0.57629531]\n",
      " [ 1.83442896 -0.62676626  0.48121026  0.86738215  1.28155851  0.64308016\n",
      "   1.0045709   0.22484296]\n",
      " [-0.08745612 -0.3653156   0.71374704  1.53317494 -0.34225107  0.3687522\n",
      "  -0.96797894 -0.11917784]]\n",
      "[[-0.89293442  0.03747081  0.94910268  4.13050579  0.13025381 -0.11197364\n",
      "   0.4786573   2.44668442]]\n",
      "[[ 0.42454909 -0.29515752  0.88010903  0.24197382  1.02841382 -0.53366261\n",
      "  -0.18075845  2.5273183 ]]\n"
     ]
    }
   ],
   "source": [
    "# Sumノード\n",
    "\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(N, D)\n",
    "print(x)\n",
    "y = np.sum(x, axis=0, keepdims=True)\n",
    "print(y)\n",
    "\n",
    "dy = np.random.randn(1, D)\n",
    "dx = np.repeat(dy, N, axis=0)\n",
    "print(dy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuatMul:\n",
    "    \"\"\"行列の積を演算するノード\n",
    "    \"\"\"\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forwrad(self, x):\n",
    "        (W,) = self.params\n",
    "        # 入力と重みの内積を演算\n",
    "        out = np.dot(x, W)\n",
    "        # backwardで使用するため入力ベクトルxを保存\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        (W,) = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "# aにbをコピーする。\n",
    "# この処理はシャロ―コピー（浅いコピー）と呼ばれ、aはbが示すアドレスを参照するようになる。\n",
    "# つまり、aの元アドレスは参照されなくなる。\n",
    "a = b\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "# a=bと同じ結果になるように見えるが\n",
    "# この処理はディープコピー(深いコピー)と呼ばれ、aは元のアドレスを保持したまま値が上書きされる。\n",
    "# aのアドレスを保持したままbの値で上書きしたい場合、このような処理を行う。\n",
    "a[...] = b\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # 親ディレクトリのファイルをインポートするための設定\n",
    "from dataset import spiral\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (300, 2)\n",
      "t (300, 3)\n"
     ]
    }
   ],
   "source": [
    "x, t = spiral.load_data()\n",
    "print(\"x\", x.shape)\n",
    "print(\"t\", t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        # 重みとバイアスの初期化\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(0)\n",
    "\n",
    "        # レイヤーの生成\n",
    "        self.layers = [Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 全ての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size = 300\n",
      "max iters = 10\n",
      "[epoch 1 | iter 10 / 10 | loss 1.13\n",
      "[epoch 2 | iter 10 / 10 | loss 1.13\n",
      "[epoch 3 | iter 10 / 10 | loss 1.12\n",
      "[epoch 4 | iter 10 / 10 | loss 1.12\n",
      "[epoch 5 | iter 10 / 10 | loss 1.11\n",
      "[epoch 6 | iter 10 / 10 | loss 1.14\n",
      "[epoch 7 | iter 10 / 10 | loss 1.16\n",
      "[epoch 8 | iter 10 / 10 | loss 1.11\n",
      "[epoch 9 | iter 10 / 10 | loss 1.12\n",
      "[epoch 10 | iter 10 / 10 | loss 1.13\n",
      "[epoch 11 | iter 10 / 10 | loss 1.12\n",
      "[epoch 12 | iter 10 / 10 | loss 1.11\n",
      "[epoch 13 | iter 10 / 10 | loss 1.09\n",
      "[epoch 14 | iter 10 / 10 | loss 1.08\n",
      "[epoch 15 | iter 10 / 10 | loss 1.04\n",
      "[epoch 16 | iter 10 / 10 | loss 1.03\n",
      "[epoch 17 | iter 10 / 10 | loss 0.96\n",
      "[epoch 18 | iter 10 / 10 | loss 0.92\n",
      "[epoch 19 | iter 10 / 10 | loss 0.92\n",
      "[epoch 20 | iter 10 / 10 | loss 0.87\n",
      "[epoch 21 | iter 10 / 10 | loss 0.85\n",
      "[epoch 22 | iter 10 / 10 | loss 0.82\n",
      "[epoch 23 | iter 10 / 10 | loss 0.79\n",
      "[epoch 24 | iter 10 / 10 | loss 0.78\n",
      "[epoch 25 | iter 10 / 10 | loss 0.82\n",
      "[epoch 26 | iter 10 / 10 | loss 0.78\n",
      "[epoch 27 | iter 10 / 10 | loss 0.76\n",
      "[epoch 28 | iter 10 / 10 | loss 0.76\n",
      "[epoch 29 | iter 10 / 10 | loss 0.78\n",
      "[epoch 30 | iter 10 / 10 | loss 0.75\n",
      "[epoch 31 | iter 10 / 10 | loss 0.78\n",
      "[epoch 32 | iter 10 / 10 | loss 0.77\n",
      "[epoch 33 | iter 10 / 10 | loss 0.77\n",
      "[epoch 34 | iter 10 / 10 | loss 0.78\n",
      "[epoch 35 | iter 10 / 10 | loss 0.75\n",
      "[epoch 36 | iter 10 / 10 | loss 0.74\n",
      "[epoch 37 | iter 10 / 10 | loss 0.76\n",
      "[epoch 38 | iter 10 / 10 | loss 0.76\n",
      "[epoch 39 | iter 10 / 10 | loss 0.73\n",
      "[epoch 40 | iter 10 / 10 | loss 0.75\n",
      "[epoch 41 | iter 10 / 10 | loss 0.76\n",
      "[epoch 42 | iter 10 / 10 | loss 0.76\n",
      "[epoch 43 | iter 10 / 10 | loss 0.76\n",
      "[epoch 44 | iter 10 / 10 | loss 0.74\n",
      "[epoch 45 | iter 10 / 10 | loss 0.75\n",
      "[epoch 46 | iter 10 / 10 | loss 0.73\n",
      "[epoch 47 | iter 10 / 10 | loss 0.72\n",
      "[epoch 48 | iter 10 / 10 | loss 0.73\n",
      "[epoch 49 | iter 10 / 10 | loss 0.72\n",
      "[epoch 50 | iter 10 / 10 | loss 0.72\n",
      "[epoch 51 | iter 10 / 10 | loss 0.72\n",
      "[epoch 52 | iter 10 / 10 | loss 0.72\n",
      "[epoch 53 | iter 10 / 10 | loss 0.74\n",
      "[epoch 54 | iter 10 / 10 | loss 0.74\n",
      "[epoch 55 | iter 10 / 10 | loss 0.72\n",
      "[epoch 56 | iter 10 / 10 | loss 0.72\n",
      "[epoch 57 | iter 10 / 10 | loss 0.71\n",
      "[epoch 58 | iter 10 / 10 | loss 0.70\n",
      "[epoch 59 | iter 10 / 10 | loss 0.72\n",
      "[epoch 60 | iter 10 / 10 | loss 0.70\n",
      "[epoch 61 | iter 10 / 10 | loss 0.71\n",
      "[epoch 62 | iter 10 / 10 | loss 0.72\n",
      "[epoch 63 | iter 10 / 10 | loss 0.70\n",
      "[epoch 64 | iter 10 / 10 | loss 0.71\n",
      "[epoch 65 | iter 10 / 10 | loss 0.73\n",
      "[epoch 66 | iter 10 / 10 | loss 0.70\n",
      "[epoch 67 | iter 10 / 10 | loss 0.71\n",
      "[epoch 68 | iter 10 / 10 | loss 0.69\n",
      "[epoch 69 | iter 10 / 10 | loss 0.70\n",
      "[epoch 70 | iter 10 / 10 | loss 0.71\n",
      "[epoch 71 | iter 10 / 10 | loss 0.68\n",
      "[epoch 72 | iter 10 / 10 | loss 0.69\n",
      "[epoch 73 | iter 10 / 10 | loss 0.67\n",
      "[epoch 74 | iter 10 / 10 | loss 0.68\n",
      "[epoch 75 | iter 10 / 10 | loss 0.67\n",
      "[epoch 76 | iter 10 / 10 | loss 0.66\n",
      "[epoch 77 | iter 10 / 10 | loss 0.69\n",
      "[epoch 78 | iter 10 / 10 | loss 0.64\n",
      "[epoch 79 | iter 10 / 10 | loss 0.68\n",
      "[epoch 80 | iter 10 / 10 | loss 0.64\n",
      "[epoch 81 | iter 10 / 10 | loss 0.64\n",
      "[epoch 82 | iter 10 / 10 | loss 0.66\n",
      "[epoch 83 | iter 10 / 10 | loss 0.62\n",
      "[epoch 84 | iter 10 / 10 | loss 0.62\n",
      "[epoch 85 | iter 10 / 10 | loss 0.61\n",
      "[epoch 86 | iter 10 / 10 | loss 0.60\n",
      "[epoch 87 | iter 10 / 10 | loss 0.60\n",
      "[epoch 88 | iter 10 / 10 | loss 0.61\n",
      "[epoch 89 | iter 10 / 10 | loss 0.59\n",
      "[epoch 90 | iter 10 / 10 | loss 0.58\n",
      "[epoch 91 | iter 10 / 10 | loss 0.56\n",
      "[epoch 92 | iter 10 / 10 | loss 0.56\n",
      "[epoch 93 | iter 10 / 10 | loss 0.54\n",
      "[epoch 94 | iter 10 / 10 | loss 0.53\n",
      "[epoch 95 | iter 10 / 10 | loss 0.53\n",
      "[epoch 96 | iter 10 / 10 | loss 0.52\n",
      "[epoch 97 | iter 10 / 10 | loss 0.51\n",
      "[epoch 98 | iter 10 / 10 | loss 0.50\n",
      "[epoch 99 | iter 10 / 10 | loss 0.48\n",
      "[epoch 100 | iter 10 / 10 | loss 0.48\n",
      "[epoch 101 | iter 10 / 10 | loss 0.46\n",
      "[epoch 102 | iter 10 / 10 | loss 0.45\n",
      "[epoch 103 | iter 10 / 10 | loss 0.45\n",
      "[epoch 104 | iter 10 / 10 | loss 0.44\n",
      "[epoch 105 | iter 10 / 10 | loss 0.44\n",
      "[epoch 106 | iter 10 / 10 | loss 0.41\n",
      "[epoch 107 | iter 10 / 10 | loss 0.40\n",
      "[epoch 108 | iter 10 / 10 | loss 0.41\n",
      "[epoch 109 | iter 10 / 10 | loss 0.40\n",
      "[epoch 110 | iter 10 / 10 | loss 0.40\n",
      "[epoch 111 | iter 10 / 10 | loss 0.38\n",
      "[epoch 112 | iter 10 / 10 | loss 0.38\n",
      "[epoch 113 | iter 10 / 10 | loss 0.36\n",
      "[epoch 114 | iter 10 / 10 | loss 0.37\n",
      "[epoch 115 | iter 10 / 10 | loss 0.35\n",
      "[epoch 116 | iter 10 / 10 | loss 0.34\n",
      "[epoch 117 | iter 10 / 10 | loss 0.34\n",
      "[epoch 118 | iter 10 / 10 | loss 0.34\n",
      "[epoch 119 | iter 10 / 10 | loss 0.33\n",
      "[epoch 120 | iter 10 / 10 | loss 0.34\n",
      "[epoch 121 | iter 10 / 10 | loss 0.32\n",
      "[epoch 122 | iter 10 / 10 | loss 0.32\n",
      "[epoch 123 | iter 10 / 10 | loss 0.31\n",
      "[epoch 124 | iter 10 / 10 | loss 0.31\n",
      "[epoch 125 | iter 10 / 10 | loss 0.30\n",
      "[epoch 126 | iter 10 / 10 | loss 0.30\n",
      "[epoch 127 | iter 10 / 10 | loss 0.28\n",
      "[epoch 128 | iter 10 / 10 | loss 0.28\n",
      "[epoch 129 | iter 10 / 10 | loss 0.28\n",
      "[epoch 130 | iter 10 / 10 | loss 0.28\n",
      "[epoch 131 | iter 10 / 10 | loss 0.27\n",
      "[epoch 132 | iter 10 / 10 | loss 0.27\n",
      "[epoch 133 | iter 10 / 10 | loss 0.27\n",
      "[epoch 134 | iter 10 / 10 | loss 0.27\n",
      "[epoch 135 | iter 10 / 10 | loss 0.27\n",
      "[epoch 136 | iter 10 / 10 | loss 0.26\n",
      "[epoch 137 | iter 10 / 10 | loss 0.26\n",
      "[epoch 138 | iter 10 / 10 | loss 0.26\n",
      "[epoch 139 | iter 10 / 10 | loss 0.25\n",
      "[epoch 140 | iter 10 / 10 | loss 0.24\n",
      "[epoch 141 | iter 10 / 10 | loss 0.24\n",
      "[epoch 142 | iter 10 / 10 | loss 0.25\n",
      "[epoch 143 | iter 10 / 10 | loss 0.24\n",
      "[epoch 144 | iter 10 / 10 | loss 0.24\n",
      "[epoch 145 | iter 10 / 10 | loss 0.23\n",
      "[epoch 146 | iter 10 / 10 | loss 0.24\n",
      "[epoch 147 | iter 10 / 10 | loss 0.23\n",
      "[epoch 148 | iter 10 / 10 | loss 0.23\n",
      "[epoch 149 | iter 10 / 10 | loss 0.22\n",
      "[epoch 150 | iter 10 / 10 | loss 0.22\n",
      "[epoch 151 | iter 10 / 10 | loss 0.22\n",
      "[epoch 152 | iter 10 / 10 | loss 0.22\n",
      "[epoch 153 | iter 10 / 10 | loss 0.22\n",
      "[epoch 154 | iter 10 / 10 | loss 0.22\n",
      "[epoch 155 | iter 10 / 10 | loss 0.22\n",
      "[epoch 156 | iter 10 / 10 | loss 0.21\n",
      "[epoch 157 | iter 10 / 10 | loss 0.21\n",
      "[epoch 158 | iter 10 / 10 | loss 0.20\n",
      "[epoch 159 | iter 10 / 10 | loss 0.21\n",
      "[epoch 160 | iter 10 / 10 | loss 0.20\n",
      "[epoch 161 | iter 10 / 10 | loss 0.20\n",
      "[epoch 162 | iter 10 / 10 | loss 0.20\n",
      "[epoch 163 | iter 10 / 10 | loss 0.21\n",
      "[epoch 164 | iter 10 / 10 | loss 0.20\n",
      "[epoch 165 | iter 10 / 10 | loss 0.20\n",
      "[epoch 166 | iter 10 / 10 | loss 0.19\n",
      "[epoch 167 | iter 10 / 10 | loss 0.19\n",
      "[epoch 168 | iter 10 / 10 | loss 0.19\n",
      "[epoch 169 | iter 10 / 10 | loss 0.19\n",
      "[epoch 170 | iter 10 / 10 | loss 0.19\n",
      "[epoch 171 | iter 10 / 10 | loss 0.19\n",
      "[epoch 172 | iter 10 / 10 | loss 0.18\n",
      "[epoch 173 | iter 10 / 10 | loss 0.18\n",
      "[epoch 174 | iter 10 / 10 | loss 0.18\n",
      "[epoch 175 | iter 10 / 10 | loss 0.18\n",
      "[epoch 176 | iter 10 / 10 | loss 0.18\n",
      "[epoch 177 | iter 10 / 10 | loss 0.18\n",
      "[epoch 178 | iter 10 / 10 | loss 0.18\n",
      "[epoch 179 | iter 10 / 10 | loss 0.17\n",
      "[epoch 180 | iter 10 / 10 | loss 0.17\n",
      "[epoch 181 | iter 10 / 10 | loss 0.18\n",
      "[epoch 182 | iter 10 / 10 | loss 0.17\n",
      "[epoch 183 | iter 10 / 10 | loss 0.18\n",
      "[epoch 184 | iter 10 / 10 | loss 0.17\n",
      "[epoch 185 | iter 10 / 10 | loss 0.17\n",
      "[epoch 186 | iter 10 / 10 | loss 0.18\n",
      "[epoch 187 | iter 10 / 10 | loss 0.17\n",
      "[epoch 188 | iter 10 / 10 | loss 0.17\n",
      "[epoch 189 | iter 10 / 10 | loss 0.17\n",
      "[epoch 190 | iter 10 / 10 | loss 0.17\n",
      "[epoch 191 | iter 10 / 10 | loss 0.16\n",
      "[epoch 192 | iter 10 / 10 | loss 0.17\n",
      "[epoch 193 | iter 10 / 10 | loss 0.16\n",
      "[epoch 194 | iter 10 / 10 | loss 0.16\n",
      "[epoch 195 | iter 10 / 10 | loss 0.16\n",
      "[epoch 196 | iter 10 / 10 | loss 0.16\n",
      "[epoch 197 | iter 10 / 10 | loss 0.16\n",
      "[epoch 198 | iter 10 / 10 | loss 0.15\n",
      "[epoch 199 | iter 10 / 10 | loss 0.16\n",
      "[epoch 200 | iter 10 / 10 | loss 0.16\n",
      "[epoch 201 | iter 10 / 10 | loss 0.15\n",
      "[epoch 202 | iter 10 / 10 | loss 0.16\n",
      "[epoch 203 | iter 10 / 10 | loss 0.16\n",
      "[epoch 204 | iter 10 / 10 | loss 0.15\n",
      "[epoch 205 | iter 10 / 10 | loss 0.16\n",
      "[epoch 206 | iter 10 / 10 | loss 0.15\n",
      "[epoch 207 | iter 10 / 10 | loss 0.15\n",
      "[epoch 208 | iter 10 / 10 | loss 0.15\n",
      "[epoch 209 | iter 10 / 10 | loss 0.15\n",
      "[epoch 210 | iter 10 / 10 | loss 0.15\n",
      "[epoch 211 | iter 10 / 10 | loss 0.15\n",
      "[epoch 212 | iter 10 / 10 | loss 0.15\n",
      "[epoch 213 | iter 10 / 10 | loss 0.15\n",
      "[epoch 214 | iter 10 / 10 | loss 0.15\n",
      "[epoch 215 | iter 10 / 10 | loss 0.15\n",
      "[epoch 216 | iter 10 / 10 | loss 0.14\n",
      "[epoch 217 | iter 10 / 10 | loss 0.14\n",
      "[epoch 218 | iter 10 / 10 | loss 0.15\n",
      "[epoch 219 | iter 10 / 10 | loss 0.14\n",
      "[epoch 220 | iter 10 / 10 | loss 0.14\n",
      "[epoch 221 | iter 10 / 10 | loss 0.14\n",
      "[epoch 222 | iter 10 / 10 | loss 0.14\n",
      "[epoch 223 | iter 10 / 10 | loss 0.14\n",
      "[epoch 224 | iter 10 / 10 | loss 0.14\n",
      "[epoch 225 | iter 10 / 10 | loss 0.14\n",
      "[epoch 226 | iter 10 / 10 | loss 0.14\n",
      "[epoch 227 | iter 10 / 10 | loss 0.14\n",
      "[epoch 228 | iter 10 / 10 | loss 0.14\n",
      "[epoch 229 | iter 10 / 10 | loss 0.13\n",
      "[epoch 230 | iter 10 / 10 | loss 0.14\n",
      "[epoch 231 | iter 10 / 10 | loss 0.13\n",
      "[epoch 232 | iter 10 / 10 | loss 0.14\n",
      "[epoch 233 | iter 10 / 10 | loss 0.13\n",
      "[epoch 234 | iter 10 / 10 | loss 0.13\n",
      "[epoch 235 | iter 10 / 10 | loss 0.13\n",
      "[epoch 236 | iter 10 / 10 | loss 0.13\n",
      "[epoch 237 | iter 10 / 10 | loss 0.14\n",
      "[epoch 238 | iter 10 / 10 | loss 0.13\n",
      "[epoch 239 | iter 10 / 10 | loss 0.13\n",
      "[epoch 240 | iter 10 / 10 | loss 0.14\n",
      "[epoch 241 | iter 10 / 10 | loss 0.13\n",
      "[epoch 242 | iter 10 / 10 | loss 0.13\n",
      "[epoch 243 | iter 10 / 10 | loss 0.13\n",
      "[epoch 244 | iter 10 / 10 | loss 0.13\n",
      "[epoch 245 | iter 10 / 10 | loss 0.13\n",
      "[epoch 246 | iter 10 / 10 | loss 0.13\n",
      "[epoch 247 | iter 10 / 10 | loss 0.13\n",
      "[epoch 248 | iter 10 / 10 | loss 0.13\n",
      "[epoch 249 | iter 10 / 10 | loss 0.13\n",
      "[epoch 250 | iter 10 / 10 | loss 0.13\n",
      "[epoch 251 | iter 10 / 10 | loss 0.13\n",
      "[epoch 252 | iter 10 / 10 | loss 0.12\n",
      "[epoch 253 | iter 10 / 10 | loss 0.12\n",
      "[epoch 254 | iter 10 / 10 | loss 0.12\n",
      "[epoch 255 | iter 10 / 10 | loss 0.12\n",
      "[epoch 256 | iter 10 / 10 | loss 0.12\n",
      "[epoch 257 | iter 10 / 10 | loss 0.12\n",
      "[epoch 258 | iter 10 / 10 | loss 0.12\n",
      "[epoch 259 | iter 10 / 10 | loss 0.13\n",
      "[epoch 260 | iter 10 / 10 | loss 0.12\n",
      "[epoch 261 | iter 10 / 10 | loss 0.13\n",
      "[epoch 262 | iter 10 / 10 | loss 0.12\n",
      "[epoch 263 | iter 10 / 10 | loss 0.12\n",
      "[epoch 264 | iter 10 / 10 | loss 0.13\n",
      "[epoch 265 | iter 10 / 10 | loss 0.12\n",
      "[epoch 266 | iter 10 / 10 | loss 0.12\n",
      "[epoch 267 | iter 10 / 10 | loss 0.12\n",
      "[epoch 268 | iter 10 / 10 | loss 0.12\n",
      "[epoch 269 | iter 10 / 10 | loss 0.11\n",
      "[epoch 270 | iter 10 / 10 | loss 0.12\n",
      "[epoch 271 | iter 10 / 10 | loss 0.12\n",
      "[epoch 272 | iter 10 / 10 | loss 0.12\n",
      "[epoch 273 | iter 10 / 10 | loss 0.12\n",
      "[epoch 274 | iter 10 / 10 | loss 0.12\n",
      "[epoch 275 | iter 10 / 10 | loss 0.11\n",
      "[epoch 276 | iter 10 / 10 | loss 0.12\n",
      "[epoch 277 | iter 10 / 10 | loss 0.12\n",
      "[epoch 278 | iter 10 / 10 | loss 0.11\n",
      "[epoch 279 | iter 10 / 10 | loss 0.11\n",
      "[epoch 280 | iter 10 / 10 | loss 0.11\n",
      "[epoch 281 | iter 10 / 10 | loss 0.11\n",
      "[epoch 282 | iter 10 / 10 | loss 0.12\n",
      "[epoch 283 | iter 10 / 10 | loss 0.11\n",
      "[epoch 284 | iter 10 / 10 | loss 0.11\n",
      "[epoch 285 | iter 10 / 10 | loss 0.11\n",
      "[epoch 286 | iter 10 / 10 | loss 0.11\n",
      "[epoch 287 | iter 10 / 10 | loss 0.11\n",
      "[epoch 288 | iter 10 / 10 | loss 0.12\n",
      "[epoch 289 | iter 10 / 10 | loss 0.11\n",
      "[epoch 290 | iter 10 / 10 | loss 0.11\n",
      "[epoch 291 | iter 10 / 10 | loss 0.11\n",
      "[epoch 292 | iter 10 / 10 | loss 0.11\n",
      "[epoch 293 | iter 10 / 10 | loss 0.11\n",
      "[epoch 294 | iter 10 / 10 | loss 0.11\n",
      "[epoch 295 | iter 10 / 10 | loss 0.12\n",
      "[epoch 296 | iter 10 / 10 | loss 0.11\n",
      "[epoch 297 | iter 10 / 10 | loss 0.12\n",
      "[epoch 298 | iter 10 / 10 | loss 0.11\n",
      "[epoch 299 | iter 10 / 10 | loss 0.11\n",
      "[epoch 300 | iter 10 / 10 | loss 0.11\n"
     ]
    }
   ],
   "source": [
    "from common.optimizer import SGD\n",
    "from dataset import spiral\n",
    "from ch01.two_layer_net import TwoLayerNet\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "data_size = len(x)\n",
    "# data_size=300\n",
    "print(f\"data size = {data_size}\")\n",
    "max_iters = data_size // batch_size\n",
    "# max_iters=10\n",
    "print(f\"max iters = {max_iters}\")\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "# 300エポックループ\n",
    "for epoch in range(max_epoch):\n",
    "    # データのシャッフル\n",
    "    # 引数で指定した数だけ、ランダムなインデックスを作成する\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "\n",
    "    # 1エポックあたり10回ループ\n",
    "    for iters in range(max_iters):\n",
    "        # x,tはそれぞれ300個のデータが格納されている。\n",
    "        # itersが1カウントアップされるごとに、30個ずつデータをミニバッチとして取り出す\n",
    "        batch_x = x[iters * batch_size : (iters + 1) * batch_size]\n",
    "        batch_t = t[iters * batch_size : (iters + 1) * batch_size]\n",
    "\n",
    "        # ミニバッチ単位で学習を行う。\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        if (iters + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print(\n",
    "                \"[epoch %d | iter %d / %d | loss %.2f\"\n",
    "                % (epoch + 1, iters + 1, max_iters, avg_loss)\n",
    "            )\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

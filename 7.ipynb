{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from common.functions import softmax\n",
    "from ch06.rnnlm import Rnnlm\n",
    "from ch06.better_rnnlm import BetterRnnlm\n",
    "\n",
    "\n",
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        \"\"\"文書生成\n",
    "\n",
    "        Args:\n",
    "            start_id (_type_): 最初に与える単語ID\n",
    "            skip_ids (_type_, optional): ここで指定した単語IDはサンプリングされないようにする. Defaults to None.\n",
    "            sample_size (int, optional): サンプリングする単語数. Defaults to 100.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        word_ids = [start_id]\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            # 単語id xの次にくる単語を示す確率を取得する\n",
    "            score = self.predict(x)\n",
    "            # 確率を正規化し確率分布pにする。\n",
    "            p = softmax(score.flatten())\n",
    "            # 確率分布pから単語idをサンプリングする。\n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "\n",
    "        return word_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 'll be seen as weekend of the company.\n",
      " mr. fleischmann made a firm all linked off the proposals sharply and former president and senate of the chairman and chief executive officer thomas lawson.\n",
      " the board is investment in his daughters.\n",
      " we need political demands.\n",
      " donald e. evans the chairman of the firm 's pilots in new products is not one of his role in an money.\n",
      " we need to see that direct competition in multiples.\n",
      " its most aggressive junk banks received on its own consent development yesterday morning he believes that the chairman\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from ch07.rnnlm_gen import RnnlmGen\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data(\"train\")\n",
    "vocab_size = len(word_to_id)\n",
    "model = RnnlmGen()\n",
    "model.load_params(\"ch06/Rnnlm.pkl\")\n",
    "\n",
    "# start文字とskip文字の設定\n",
    "start_word = \"you\"\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = [\"N\", \"<unk>\", \"$\"]\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# 文章生成\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = \" \".join([id_to_word[w] for w in word_ids])\n",
    "txt = txt.replace(\" <eos>\", \".\\n\")\n",
    "print(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "[ 3  0  2  0  0 11  5]\n",
      "[ 6  0 11  7  5]\n",
      "71+118 \n",
      "_189 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from dataset import sequence\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data(\"addition.txt\", seed=1984)\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "print(t_train[0])\n",
    "print(\"\".join([id_to_char[c] for c in x_train[0]]))\n",
    "print(\"\".join([id_to_char[c] for c in t_train[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "from common.time_layers import TimeAffine, TimeEmbedding, TimeLSTM, TimeSoftmaxWithLoss\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype(\"f\")\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype(\"f\")\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype(\"f\")\n",
    "        lstm_b = np.zeros(4 * H).astype(\"f\")\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[:, -1, :]\n",
    "\n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype(\"f\")\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype(\"f\")\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype(\"f\")\n",
    "        lstm_b = np.zeros(4 * H).astype(\"f\")\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype(\"f\")\n",
    "        affine_b = np.zeros(V).astype(\"f\")\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(h)\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            h (_type_): Encoderから受け取る隠れ状態\n",
    "            start_id (_type_): 最初にあたえる文字ID\n",
    "            sample_size (_type_): 生成する文字数\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape(1, 1)\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "\n",
    "        h = self.decoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss=self.softmax.forward(score,decoder_ts)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self,dout=1):\n",
    "        dout=self.softmax.backward(dout)\n",
    "        dh=self.decoder.backward(dout)\n",
    "        dout=self.encoder.backward(dh)\n",
    "        return dout\n",
    "\n",
    "    def generate(self, xs,start_id,sample_size):\n",
    "        h=self.encodre.forward(xs)\n",
    "        sampled=self.decoder.generate(h,start_id,sample_size)\n",
    "        return sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 2.56\n",
      "| epoch 1 |  iter 21 / 351 | time 0[s] | loss 2.53\n",
      "| epoch 1 |  iter 41 / 351 | time 1[s] | loss 2.17\n",
      "| epoch 1 |  iter 61 / 351 | time 2[s] | loss 1.96\n",
      "| epoch 1 |  iter 81 / 351 | time 3[s] | loss 1.92\n",
      "| epoch 1 |  iter 101 / 351 | time 4[s] | loss 1.87\n",
      "| epoch 1 |  iter 121 / 351 | time 4[s] | loss 1.85\n",
      "| epoch 1 |  iter 141 / 351 | time 5[s] | loss 1.83\n",
      "| epoch 1 |  iter 161 / 351 | time 6[s] | loss 1.79\n",
      "| epoch 1 |  iter 181 / 351 | time 7[s] | loss 1.77\n",
      "| epoch 1 |  iter 201 / 351 | time 8[s] | loss 1.77\n",
      "| epoch 1 |  iter 221 / 351 | time 9[s] | loss 1.76\n",
      "| epoch 1 |  iter 241 / 351 | time 9[s] | loss 1.76\n",
      "| epoch 1 |  iter 261 / 351 | time 10[s] | loss 1.76\n",
      "| epoch 1 |  iter 281 / 351 | time 11[s] | loss 1.75\n",
      "| epoch 1 |  iter 301 / 351 | time 12[s] | loss 1.74\n",
      "| epoch 1 |  iter 321 / 351 | time 13[s] | loss 1.75\n",
      "| epoch 1 |  iter 341 / 351 | time 14[s] | loss 1.74\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 100 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 100 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 100 \n",
      "---\n",
      "val acc 0.180%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.74\n",
      "| epoch 2 |  iter 21 / 351 | time 0[s] | loss 1.73\n",
      "| epoch 2 |  iter 41 / 351 | time 1[s] | loss 1.74\n",
      "| epoch 2 |  iter 61 / 351 | time 2[s] | loss 1.74\n",
      "| epoch 2 |  iter 81 / 351 | time 3[s] | loss 1.73\n",
      "| epoch 2 |  iter 101 / 351 | time 4[s] | loss 1.73\n",
      "| epoch 2 |  iter 121 / 351 | time 5[s] | loss 1.72\n",
      "| epoch 2 |  iter 141 / 351 | time 5[s] | loss 1.71\n",
      "| epoch 2 |  iter 161 / 351 | time 6[s] | loss 1.71\n",
      "| epoch 2 |  iter 181 / 351 | time 7[s] | loss 1.71\n",
      "| epoch 2 |  iter 201 / 351 | time 8[s] | loss 1.70\n",
      "| epoch 2 |  iter 221 / 351 | time 8[s] | loss 1.71\n",
      "| epoch 2 |  iter 241 / 351 | time 9[s] | loss 1.70\n",
      "| epoch 2 |  iter 261 / 351 | time 10[s] | loss 1.69\n",
      "| epoch 2 |  iter 281 / 351 | time 11[s] | loss 1.69\n",
      "| epoch 2 |  iter 301 / 351 | time 11[s] | loss 1.69\n",
      "| epoch 2 |  iter 321 / 351 | time 12[s] | loss 1.68\n",
      "| epoch 2 |  iter 341 / 351 | time 13[s] | loss 1.67\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 994 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 700 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 100 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 400 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1000\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1544\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 400 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 400 \n",
      "---\n",
      "val acc 0.220%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 1.66\n",
      "| epoch 3 |  iter 21 / 351 | time 0[s] | loss 1.66\n",
      "| epoch 3 |  iter 41 / 351 | time 1[s] | loss 1.65\n",
      "| epoch 3 |  iter 61 / 351 | time 2[s] | loss 1.63\n",
      "| epoch 3 |  iter 81 / 351 | time 3[s] | loss 1.62\n",
      "| epoch 3 |  iter 101 / 351 | time 4[s] | loss 1.62\n",
      "| epoch 3 |  iter 121 / 351 | time 4[s] | loss 1.60\n",
      "| epoch 3 |  iter 141 / 351 | time 5[s] | loss 1.59\n",
      "| epoch 3 |  iter 161 / 351 | time 6[s] | loss 1.57\n",
      "| epoch 3 |  iter 181 / 351 | time 7[s] | loss 1.57\n",
      "| epoch 3 |  iter 201 / 351 | time 7[s] | loss 1.56\n",
      "| epoch 3 |  iter 221 / 351 | time 8[s] | loss 1.54\n",
      "| epoch 3 |  iter 241 / 351 | time 9[s] | loss 1.52\n",
      "| epoch 3 |  iter 261 / 351 | time 10[s] | loss 1.52\n",
      "| epoch 3 |  iter 281 / 351 | time 10[s] | loss 1.52\n",
      "| epoch 3 |  iter 301 / 351 | time 11[s] | loss 1.50\n",
      "| epoch 3 |  iter 321 / 351 | time 12[s] | loss 1.49\n",
      "| epoch 3 |  iter 341 / 351 | time 13[s] | loss 1.48\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 108 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1001\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 648 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 138 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 448 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 848 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1011\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1373\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 868 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 348 \n",
      "---\n",
      "val acc 0.560%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 1.47\n",
      "| epoch 4 |  iter 21 / 351 | time 0[s] | loss 1.46\n",
      "| epoch 4 |  iter 41 / 351 | time 2[s] | loss 1.44\n",
      "| epoch 4 |  iter 61 / 351 | time 2[s] | loss 1.43\n",
      "| epoch 4 |  iter 81 / 351 | time 3[s] | loss 1.42\n",
      "| epoch 4 |  iter 101 / 351 | time 4[s] | loss 1.41\n",
      "| epoch 4 |  iter 121 / 351 | time 5[s] | loss 1.40\n",
      "| epoch 4 |  iter 141 / 351 | time 6[s] | loss 1.40\n",
      "| epoch 4 |  iter 161 / 351 | time 6[s] | loss 1.38\n",
      "| epoch 4 |  iter 181 / 351 | time 7[s] | loss 1.38\n",
      "| epoch 4 |  iter 201 / 351 | time 8[s] | loss 1.37\n",
      "| epoch 4 |  iter 221 / 351 | time 9[s] | loss 1.35\n",
      "| epoch 4 |  iter 241 / 351 | time 10[s] | loss 1.33\n",
      "| epoch 4 |  iter 261 / 351 | time 10[s] | loss 1.33\n",
      "| epoch 4 |  iter 281 / 351 | time 12[s] | loss 1.33\n",
      "| epoch 4 |  iter 301 / 351 | time 13[s] | loss 1.32\n",
      "| epoch 4 |  iter 321 / 351 | time 13[s] | loss 1.31\n",
      "| epoch 4 |  iter 341 / 351 | time 14[s] | loss 1.30\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 146 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1189\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[92m☑\u001b[0m 666 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 162 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 432 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 866 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1002\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1406\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 862 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 202 \n",
      "---\n",
      "val acc 1.060%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 1.28\n",
      "| epoch 5 |  iter 21 / 351 | time 0[s] | loss 1.29\n",
      "| epoch 5 |  iter 41 / 351 | time 1[s] | loss 1.28\n",
      "| epoch 5 |  iter 61 / 351 | time 2[s] | loss 1.27\n",
      "| epoch 5 |  iter 81 / 351 | time 3[s] | loss 1.27\n",
      "| epoch 5 |  iter 101 / 351 | time 3[s] | loss 1.26\n",
      "| epoch 5 |  iter 121 / 351 | time 4[s] | loss 1.26\n",
      "| epoch 5 |  iter 141 / 351 | time 5[s] | loss 1.27\n",
      "| epoch 5 |  iter 161 / 351 | time 6[s] | loss 1.26\n",
      "| epoch 5 |  iter 181 / 351 | time 7[s] | loss 1.25\n",
      "| epoch 5 |  iter 201 / 351 | time 7[s] | loss 1.23\n",
      "| epoch 5 |  iter 221 / 351 | time 9[s] | loss 1.22\n",
      "| epoch 5 |  iter 241 / 351 | time 10[s] | loss 1.21\n",
      "| epoch 5 |  iter 261 / 351 | time 11[s] | loss 1.21\n",
      "| epoch 5 |  iter 281 / 351 | time 12[s] | loss 1.21\n",
      "| epoch 5 |  iter 301 / 351 | time 13[s] | loss 1.20\n",
      "| epoch 5 |  iter 321 / 351 | time 13[s] | loss 1.19\n",
      "| epoch 5 |  iter 341 / 351 | time 14[s] | loss 1.18\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 145 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1168\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 665 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 192 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 431 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 895 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1015\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1493\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 891 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 221 \n",
      "---\n",
      "val acc 2.260%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 1.17\n",
      "| epoch 6 |  iter 21 / 351 | time 1[s] | loss 1.17\n",
      "| epoch 6 |  iter 41 / 351 | time 2[s] | loss 1.18\n",
      "| epoch 6 |  iter 61 / 351 | time 3[s] | loss 1.17\n",
      "| epoch 6 |  iter 81 / 351 | time 4[s] | loss 1.16\n",
      "| epoch 6 |  iter 101 / 351 | time 5[s] | loss 1.16\n",
      "| epoch 6 |  iter 121 / 351 | time 6[s] | loss 1.16\n",
      "| epoch 6 |  iter 141 / 351 | time 8[s] | loss 1.14\n",
      "| epoch 6 |  iter 161 / 351 | time 9[s] | loss 1.14\n",
      "| epoch 6 |  iter 181 / 351 | time 11[s] | loss 1.13\n",
      "| epoch 6 |  iter 201 / 351 | time 12[s] | loss 1.15\n",
      "| epoch 6 |  iter 221 / 351 | time 13[s] | loss 1.12\n",
      "| epoch 6 |  iter 241 / 351 | time 14[s] | loss 1.13\n",
      "| epoch 6 |  iter 261 / 351 | time 15[s] | loss 1.12\n",
      "| epoch 6 |  iter 281 / 351 | time 15[s] | loss 1.12\n",
      "| epoch 6 |  iter 301 / 351 | time 17[s] | loss 1.12\n",
      "| epoch 6 |  iter 321 / 351 | time 17[s] | loss 1.15\n",
      "| epoch 6 |  iter 341 / 351 | time 18[s] | loss 1.21\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 156 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1169\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 660 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 164 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 400 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 846 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1011\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1410\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 860 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 207 \n",
      "---\n",
      "val acc 2.060%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 1.12\n",
      "| epoch 7 |  iter 21 / 351 | time 0[s] | loss 1.11\n",
      "| epoch 7 |  iter 41 / 351 | time 1[s] | loss 1.09\n",
      "| epoch 7 |  iter 61 / 351 | time 2[s] | loss 1.10\n",
      "| epoch 7 |  iter 81 / 351 | time 3[s] | loss 1.09\n",
      "| epoch 7 |  iter 101 / 351 | time 4[s] | loss 1.08\n",
      "| epoch 7 |  iter 121 / 351 | time 5[s] | loss 1.08\n",
      "| epoch 7 |  iter 141 / 351 | time 6[s] | loss 1.08\n",
      "| epoch 7 |  iter 161 / 351 | time 7[s] | loss 1.09\n",
      "| epoch 7 |  iter 181 / 351 | time 7[s] | loss 1.07\n",
      "| epoch 7 |  iter 201 / 351 | time 8[s] | loss 1.06\n",
      "| epoch 7 |  iter 221 / 351 | time 9[s] | loss 1.08\n",
      "| epoch 7 |  iter 241 / 351 | time 10[s] | loss 1.05\n",
      "| epoch 7 |  iter 261 / 351 | time 11[s] | loss 1.06\n",
      "| epoch 7 |  iter 281 / 351 | time 12[s] | loss 1.06\n",
      "| epoch 7 |  iter 301 / 351 | time 13[s] | loss 1.06\n",
      "| epoch 7 |  iter 321 / 351 | time 14[s] | loss 1.05\n",
      "| epoch 7 |  iter 341 / 351 | time 14[s] | loss 1.04\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 166 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1166\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 695 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 161 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 430 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 893 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1058\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1445\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 228 \n",
      "---\n",
      "val acc 2.340%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 1.11\n",
      "| epoch 8 |  iter 21 / 351 | time 1[s] | loss 1.08\n",
      "| epoch 8 |  iter 41 / 351 | time 2[s] | loss 1.04\n",
      "| epoch 8 |  iter 61 / 351 | time 2[s] | loss 1.04\n",
      "| epoch 8 |  iter 81 / 351 | time 3[s] | loss 1.04\n",
      "| epoch 8 |  iter 101 / 351 | time 4[s] | loss 1.04\n",
      "| epoch 8 |  iter 121 / 351 | time 6[s] | loss 1.03\n",
      "| epoch 8 |  iter 141 / 351 | time 6[s] | loss 1.03\n",
      "| epoch 8 |  iter 161 / 351 | time 7[s] | loss 1.03\n",
      "| epoch 8 |  iter 181 / 351 | time 8[s] | loss 1.02\n",
      "| epoch 8 |  iter 201 / 351 | time 9[s] | loss 1.02\n",
      "| epoch 8 |  iter 221 / 351 | time 9[s] | loss 1.01\n",
      "| epoch 8 |  iter 241 / 351 | time 10[s] | loss 1.01\n",
      "| epoch 8 |  iter 261 / 351 | time 11[s] | loss 1.01\n",
      "| epoch 8 |  iter 281 / 351 | time 12[s] | loss 1.03\n",
      "| epoch 8 |  iter 301 / 351 | time 12[s] | loss 1.01\n",
      "| epoch 8 |  iter 321 / 351 | time 13[s] | loss 1.01\n",
      "| epoch 8 |  iter 341 / 351 | time 14[s] | loss 1.02\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 158 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1160\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 668 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[92m☑\u001b[0m 163 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 431 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 859 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1410\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 868 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 227 \n",
      "---\n",
      "val acc 5.420%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.98\n",
      "| epoch 9 |  iter 21 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 9 |  iter 41 / 351 | time 1[s] | loss 1.01\n",
      "| epoch 9 |  iter 61 / 351 | time 2[s] | loss 0.99\n",
      "| epoch 9 |  iter 81 / 351 | time 3[s] | loss 0.99\n",
      "| epoch 9 |  iter 101 / 351 | time 4[s] | loss 0.99\n",
      "| epoch 9 |  iter 121 / 351 | time 5[s] | loss 1.01\n",
      "| epoch 9 |  iter 141 / 351 | time 5[s] | loss 1.03\n",
      "| epoch 9 |  iter 161 / 351 | time 6[s] | loss 0.98\n",
      "| epoch 9 |  iter 181 / 351 | time 7[s] | loss 0.99\n",
      "| epoch 9 |  iter 201 / 351 | time 7[s] | loss 0.99\n",
      "| epoch 9 |  iter 221 / 351 | time 8[s] | loss 1.02\n",
      "| epoch 9 |  iter 241 / 351 | time 9[s] | loss 1.00\n",
      "| epoch 9 |  iter 261 / 351 | time 10[s] | loss 0.98\n",
      "| epoch 9 |  iter 281 / 351 | time 10[s] | loss 0.98\n",
      "| epoch 9 |  iter 301 / 351 | time 11[s] | loss 0.97\n",
      "| epoch 9 |  iter 321 / 351 | time 12[s] | loss 0.97\n",
      "| epoch 9 |  iter 341 / 351 | time 13[s] | loss 0.97\n",
      "Q 77+85  \n",
      "T 162 \n",
      "\u001b[91m☒\u001b[0m 167 \n",
      "---\n",
      "Q 975+164\n",
      "T 1139\n",
      "\u001b[91m☒\u001b[0m 1128\n",
      "---\n",
      "Q 582+84 \n",
      "T 666 \n",
      "\u001b[91m☒\u001b[0m 677 \n",
      "---\n",
      "Q 8+155  \n",
      "T 163 \n",
      "\u001b[91m☒\u001b[0m 173 \n",
      "---\n",
      "Q 367+55 \n",
      "T 422 \n",
      "\u001b[91m☒\u001b[0m 418 \n",
      "---\n",
      "Q 600+257\n",
      "T 857 \n",
      "\u001b[91m☒\u001b[0m 875 \n",
      "---\n",
      "Q 761+292\n",
      "T 1053\n",
      "\u001b[91m☒\u001b[0m 1039\n",
      "---\n",
      "Q 830+597\n",
      "T 1427\n",
      "\u001b[91m☒\u001b[0m 1429\n",
      "---\n",
      "Q 26+838 \n",
      "T 864 \n",
      "\u001b[91m☒\u001b[0m 867 \n",
      "---\n",
      "Q 143+93 \n",
      "T 236 \n",
      "\u001b[91m☒\u001b[0m 228 \n",
      "---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mセル5 を /home/yuki/clone/deep-learning-from-scratch-2/7.ipynb\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     question,correct\u001b[39m=\u001b[39mx_test[[i]],t_test[[i]]\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     verbose\u001b[39m=\u001b[39mi\u001b[39m<\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     correct_num\u001b[39m+\u001b[39m\u001b[39m=\u001b[39meval_seq2seq(model,question,correct, id_to_char,verbose)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m acc\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(correct_num)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(x_test)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7265766572656e745f776f7a6e69616b222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32302e30345c5c686f6d655c5c79756b695c5c636c6f6e655c5c646565702d6c6561726e696e672d66726f6d2d736372617463682d32227d/home/yuki/clone/deep-learning-from-scratch-2/7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m acc_list\u001b[39m.\u001b[39mappend(acc)\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/common/util.py:233\u001b[0m, in \u001b[0;36meval_seq2seq\u001b[0;34m(model, question, correct, id_to_char, verbose, is_reverse)\u001b[0m\n\u001b[1;32m    231\u001b[0m start_id \u001b[39m=\u001b[39m correct[\u001b[39m0\u001b[39m]\n\u001b[1;32m    232\u001b[0m correct \u001b[39m=\u001b[39m correct[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 233\u001b[0m guess \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(question, start_id, \u001b[39mlen\u001b[39;49m(correct))\n\u001b[1;32m    235\u001b[0m \u001b[39m# 文字列へ変換\u001b[39;00m\n\u001b[1;32m    236\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([id_to_char[\u001b[39mint\u001b[39m(c)] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m question\u001b[39m.\u001b[39mflatten()])\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/ch07/seq2seq.py:119\u001b[0m, in \u001b[0;36mSeq2seq.generate\u001b[0;34m(self, xs, start_id, sample_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, xs, start_id, sample_size):\n\u001b[1;32m    118\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mforward(xs)\n\u001b[0;32m--> 119\u001b[0m     sampled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mgenerate(h, start_id, sample_size)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m sampled\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/ch07/seq2seq.py:84\u001b[0m, in \u001b[0;36mDecoder.generate\u001b[0;34m(self, h, start_id, sample_size)\u001b[0m\n\u001b[1;32m     82\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(sample_id)\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m     83\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed\u001b[39m.\u001b[39mforward(x)\n\u001b[0;32m---> 84\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm\u001b[39m.\u001b[39;49mforward(out)\n\u001b[1;32m     85\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine\u001b[39m.\u001b[39mforward(out)\n\u001b[1;32m     87\u001b[0m sample_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(score\u001b[39m.\u001b[39mflatten())\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/common/time_layers.py:193\u001b[0m, in \u001b[0;36mTimeLSTM.forward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[1;32m    192\u001b[0m     layer \u001b[39m=\u001b[39m LSTM(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(xs[:, t, :], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc)\n\u001b[1;32m    194\u001b[0m     hs[:, t, :] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh\n\u001b[1;32m    196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mappend(layer)\n",
      "File \u001b[0;32m~/clone/deep-learning-from-scratch-2/common/time_layers.py:114\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x, h_prev, c_prev)\u001b[0m\n\u001b[1;32m    111\u001b[0m Wx, Wh, b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n\u001b[1;32m    112\u001b[0m N, H \u001b[39m=\u001b[39m h_prev\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 114\u001b[0m A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(x, Wx) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mdot(h_prev, Wh) \u001b[39m+\u001b[39m b\n\u001b[1;32m    116\u001b[0m f \u001b[39m=\u001b[39m A[:, :H]\n\u001b[1;32m    117\u001b[0m g \u001b[39m=\u001b[39m A[:, H:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mH]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_perplexity, eval_seq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "(x_train, t_train),(x_test, t_test) = sequence.load_data(\"addition.txt\")\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size=16\n",
    "hidden_size=128\n",
    "batch_size=128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "\n",
    "# モデル、オプティマイザ、トレーナーの生成\n",
    "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list=[]\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train,max_epoch=1,batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num=0\n",
    "    for i in range(len(x_test)):\n",
    "        question,correct=x_test[[i]],t_test[[i]]\n",
    "        verbose=i<10\n",
    "        correct_num+=eval_seq2seq(model,question,correct, id_to_char,verbose)\n",
    "    acc=float(correct_num)/len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print(\"val acc %.3f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
